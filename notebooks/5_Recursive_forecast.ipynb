{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "\n",
    "from feature_engine.imputation import (\n",
    "    ArbitraryNumberImputer,\n",
    "    CategoricalImputer,\n",
    ")\n",
    "from feature_engine.selection import DropConstantFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from sktime.transformations.series.summarize import WindowSummarizer\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import set_config\n",
    "from sktime.transformations.series.summarize import WindowSummarizer\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "\n",
    "set_config(transform_output=\"pandas\")\n",
    "sns.set_context(\"talk\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_COLUMNS = [\"holiday_name\"]\n",
    "NUMERICAL_COLUMNS = [\n",
    "    \"holiday\",\n",
    "    \"shutdown\",\n",
    "    \"mini_shutdown\",\n",
    "    \"shops_closed\",\n",
    "    \"winter_school_holidays\",\n",
    "    \"school_holidays\",\n",
    "    \"blackout\",\n",
    "    \"mov_change\",\n",
    "    \"frankfurt_shutdown\",\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day_of_month\",\n",
    "    \"day_of_week\",\n",
    "    \"day_of_year\",\n",
    "    \"weekend\",\n",
    "]\n",
    "DATE_COLUMNS = [\"date\"]\n",
    "\n",
    "TARGET_COLUMNS = \"orders\"\n",
    "FEATRURE_COLUMNS = NUMERICAL_COLUMNS + CATEGORICAL_COLUMNS  # + DATE_COLUMNS\n",
    "TRAIN_LEVEL = [\"warehouse\"]\n",
    "MODEL_PATH = \"model_registry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Pipelines and transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        # ('drop_na_lags', DropMissingData(variables=new_cols)),\n",
    "        # Impute missing categorical except mean-encoded ones, normally happens in the test data\n",
    "        (\n",
    "            \"missing_categoricals\",\n",
    "            CategoricalImputer(\n",
    "                imputation_method=\"missing\", variables=CATEGORICAL_COLUMNS\n",
    "            ),\n",
    "        ),\n",
    "        # Impute 0 for missing numericals\n",
    "        (\n",
    "            \"missing_numerical\",\n",
    "            ArbitraryNumberImputer(variables=NUMERICAL_COLUMNS, arbitrary_number=0),\n",
    "        ),\n",
    "        # OneHotEncode the rest of categorical\n",
    "        (\n",
    "            \"onehot_encoding\",\n",
    "            OneHotEncoder(\n",
    "                top_categories=6,\n",
    "                variables=CATEGORICAL_COLUMNS,\n",
    "                ignore_format=True,\n",
    "            ),\n",
    "        ),\n",
    "        # Drop Constant features\n",
    "        (\"drop_constant\", DropConstantFeatures(tol=1)),\n",
    "        (\n",
    "            \"xgb\",\n",
    "            XGBRegressor(\n",
    "                learning_rate=0.05,\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                alpha=0.5,\n",
    "                # lambda=1,\n",
    "                min_child_weight=5,\n",
    "                gamma=0,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_window_feats = WindowSummarizer(\n",
    "    lag_feature={\n",
    "        \"lag\": [1, 7, 365],  # Lag features.\n",
    "        \"mean\": [[1, 7], [365, 7]],  # [[lag, window size]]\n",
    "    },\n",
    "    target_cols=\"y\",\n",
    "    # truncate=\"bfill\",  # Backfill missing values from lagging and windowing.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_window_feats_2 = WindowSummarizer(\n",
    "    lag_feature={\n",
    "        \"lag\": [1, 7],  # Lag features.\n",
    "        \"mean\": [[1, 7]],  # [[lag, window size]]\n",
    "    },\n",
    "    target_cols=\"y\",\n",
    "    # truncate=\"bfill\",  # Backfill missing values from lagging and windowing.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_window_feats_3 = WindowSummarizer(\n",
    "    lag_feature={\n",
    "        \"lag\": [1, 7, 365],  # Lag features.\n",
    "        \"mean\": [[1, 7], [365, 7]],  # [[lag, window size]]\n",
    "    },\n",
    "    target_cols=\"y\",\n",
    "    truncate=\"bfill\",  # Backfill missing values from lagging and windowing.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Batch traning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_core(\n",
    "    df: pd.DataFrame,\n",
    "    train_level: list,\n",
    "    feature_columns: list,\n",
    "    target_column: str,\n",
    "    pipeline: Pipeline,\n",
    "    model_dir: str = \"model_registry\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"This function run the train for a single level.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A dataframe including level columns, features, and target columns\n",
    "        train_level (list): training level i.e. `['warehouse', 'item_class']`\n",
    "        feature_columns (list): Feature columns.\n",
    "        target_column (str): traget columns should be a single string.\n",
    "        model_dir (str, optional): Name of the directory for saving the model pickel files. Defaults to \"model_registry\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A single row dataframe including the train level, model registry path, and error if we have any.\n",
    "    \"\"\"\n",
    "\n",
    "    e = \"\"\n",
    "    train_level_value = df[train_level].iloc[0, :].to_list()\n",
    "\n",
    "    try:\n",
    "        X = df[feature_columns]\n",
    "        y = df[target_column]\n",
    "\n",
    "        model = pipeline.fit(X, y)\n",
    "\n",
    "        # saving the model to model registry\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        model_string = \"_\".join(\n",
    "            [f\"{a}_{b}\" for a, b in zip(train_level, train_level_value)]\n",
    "        )\n",
    "        model_path = os.path.join(model_dir, f\"model_{model_string}.pkl\")\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"e\")\n",
    "\n",
    "    train_df = pd.DataFrame(\n",
    "        [train_level_value + [\"_\".join(train_level)] + [model_path] + [e]],\n",
    "        columns=[*train_level, \"train_level\", \"model_path\", \"error\"],\n",
    "    )\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_model_core(\n",
    "    df: pd.DataFrame,\n",
    "    train_level: list,\n",
    "    feature_columns: list,\n",
    "    target_column: str,\n",
    "    model_dir: str = \"model_registry\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Loading a model from model registry and inference.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A dataframe including level columns, features, and target columns\n",
    "        train_level (list): training level i.e. `['warehouse', 'item_class']`\n",
    "        feature_columns (list): Feature columns.\n",
    "        target_column (str): traget columns should be a single string.\n",
    "        model_dir (str, optional): Name of the directory for saving the model pickel files. Defaults to \"model_registry\".\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe including all input dataframe with forecast and error column added at the end.\n",
    "    \"\"\"\n",
    "    e = \"\"\n",
    "    df = df.copy()\n",
    "    train_level_value = df[train_level].iloc[0, :].to_list()\n",
    "\n",
    "    try:\n",
    "        X = df[feature_columns]\n",
    "        y = df[target_column]\n",
    "\n",
    "        model_string = \"_\".join(\n",
    "            [f\"{a}_{b}\" for a, b in zip(train_level, train_level_value)]\n",
    "        )\n",
    "        model_path = os.path.join(model_dir, f\"model_{model_string}.pkl\")\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        df.loc[:, \"forecast\"] = model.predict(X)\n",
    "        df.loc[:, \"model_path\"] = model_path\n",
    "\n",
    "    except Exception as e:\n",
    "        df.loc[:, \"forecast\"] = np.null\n",
    "        df.loc[:, \"model_path\"] = np.null\n",
    "\n",
    "    df.loc[:, \"error\"] = e\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Recursive forecasting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_sktime(\n",
    "    df,\n",
    "    feature_columns,\n",
    "    target_columns,\n",
    "    max_lag=365,\n",
    "    pipeline=None,\n",
    "    pipeline_lags_transformer=None,\n",
    "    imputation=\"mean\",\n",
    "):\n",
    "\n",
    "    print(f\"warehouse: {df.warehouse.unique()[0]}\")\n",
    "\n",
    "    df.rename(columns={target_columns: \"y\"}, inplace=True)\n",
    "\n",
    "    df_train = df.query(\"split=='train'\")[feature_columns + [\"y\", \"missing\"]].copy()\n",
    "    df_test = df.query(\"split=='test'\")[feature_columns + [\"y\", \"missing\"]].copy()\n",
    "    df_predict = df[\n",
    "        (df.split == \"test\")\n",
    "        | (\n",
    "            (df.split == \"train\")\n",
    "            & (df.index < (df_train.index.max() - datetime.timedelta(days=max_lag)))\n",
    "        )\n",
    "    ]\n",
    "    if imputation == \"mean\":\n",
    "        df_train[\"y\"] = np.where(\n",
    "            df_train[\"missing\"] == 1, df_train[\"y\"].mean(), df_train[\"y\"]\n",
    "        )\n",
    "    elif imputation == \"zero\":\n",
    "        df_train[\"y\"] = np.where(df_train[\"missing\"] == 1, 0, df_train[\"y\"])\n",
    "    else:\n",
    "        raise ValueError(\"Imputation method not supported\")\n",
    "\n",
    "    df_train_lags_only = pipeline_lags_transformer.fit_transform(df_train[[\"y\"]])\n",
    "\n",
    "    new_cols = list(df_train_lags_only.columns)\n",
    "\n",
    "    df_train_lags = pd.concat([df_train, df_train_lags_only], axis=1)\n",
    "    # drop missing targets\n",
    "    df_train_lags = df_train_lags.query(\"missing == 0\")\n",
    "\n",
    "    df_train_lags.dropna(subset=new_cols, how=\"any\", inplace=True)\n",
    "    \n",
    "    X_train = df_train_lags[feature_columns + new_cols]\n",
    "    y_train = df_train_lags[\"y\"]\n",
    "\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "\n",
    "    for forecast_time in df_test.index:\n",
    "        df_predict_lags_only = pipeline_lags_transformer.transform(df_predict[[\"y\"]])\n",
    "        X_test = pd.concat([df_predict, df_predict_lags_only], axis=1)\n",
    "        X_test_ = X_test.loc[[forecast_time]]\n",
    "        y_pred = model.predict(X_test_[feature_columns + new_cols])\n",
    "        df_predict.loc[[forecast_time], \"y\"] = y_pred\n",
    "\n",
    "    df_predict.rename(columns={\"y\": target_columns}, inplace=True)\n",
    "\n",
    "    return df_predict.query(\"split=='test' and missing==0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 Loading data and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    PATH = \"./data/\"\n",
    "    df_train = pd.read_csv(\n",
    "        f\"{PATH}train_new.csv\", parse_dates=[\"date\"], index_col=\"date\"\n",
    "    )\n",
    "    df_train[\"split\"] = \"train\"\n",
    "\n",
    "    df_test = pd.read_csv(f\"{PATH}test_new.csv\", parse_dates=[\"date\"], index_col=\"date\")\n",
    "    df_test[\"split\"] = \"test\"\n",
    "\n",
    "    cols = df_train.columns\n",
    "    df_test = df_test[cols]\n",
    "\n",
    "    df = pd.concat([df_train, df_test], axis=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data(test_days=90):\n",
    "    PATH = \"./data/\"\n",
    "    df_train = pd.read_csv(\n",
    "        f\"{PATH}train_new.csv\", parse_dates=[\"date\"], index_col=\"date\"\n",
    "    )\n",
    "    df_train[\"orders_copy\"] = df_train[\"orders\"]\n",
    "\n",
    "    max_date = df_train.index.max() - datetime.timedelta(days=test_days)\n",
    "    df_train.loc[df_train.index > max_date, \"split\"] = \"test\"\n",
    "    df_train.loc[df_train.index <= max_date, \"split\"] = \"train\"\n",
    "    df_train[\"orders\"] = np.where(\n",
    "        df_train[\"split\"] == \"test\", np.nan, df_train[\"orders\"]\n",
    "    )\n",
    "\n",
    "    return df_train\n",
    "\n",
    "\n",
    "def wmape(y_true, y_pred):\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Test models on holdout period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/train_new.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_test_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_days\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m, in \u001b[0;36mload_test_data\u001b[1;34m(test_days)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_test_data\u001b[39m(test_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m90\u001b[39m):\n\u001b[0;32m      2\u001b[0m     PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     df_train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mtrain_new.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     df_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morders_copy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m     max_date \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtimedelta(days\u001b[38;5;241m=\u001b[39mtest_days)\n",
      "File \u001b[1;32mc:\\Users\\alire\\OneDrive\\personal_projects\\venv\\kaggle\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alire\\OneDrive\\personal_projects\\venv\\kaggle\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\alire\\OneDrive\\personal_projects\\venv\\kaggle\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alire\\OneDrive\\personal_projects\\venv\\kaggle\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\alire\\OneDrive\\personal_projects\\venv\\kaggle\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/train_new.csv'"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "df = load_test_data(test_days=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Recursive forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast = (\n",
    "    df.groupby(TRAIN_LEVEL)\n",
    "    .apply(\n",
    "        train_predict_sktime,\n",
    "        FEATRURE_COLUMNS,\n",
    "        TARGET_COLUMNS,\n",
    "        max_lag=365,\n",
    "        pipeline=pipeline,\n",
    "        pipeline_lags_transformer=lag_window_feats,\n",
    "        imputation=\"mean\",\n",
    "    )\n",
    "    .drop(columns=[\"warehouse\"])\n",
    "    .reset_index()\n",
    "    .query(\"missing==0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_2 = (\n",
    "    df.groupby(TRAIN_LEVEL)\n",
    "    .apply(\n",
    "        train_predict_sktime,\n",
    "        FEATRURE_COLUMNS,\n",
    "        TARGET_COLUMNS,\n",
    "        max_lag=7,\n",
    "        pipeline=pipeline,\n",
    "        pipeline_lags_transformer=lag_window_feats_2,\n",
    "        imputation=\"mean\",\n",
    "    )\n",
    "    .drop(columns=[\"warehouse\"])\n",
    "    .reset_index()\n",
    "    .query(\"missing==0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_3 = (\n",
    "    df.groupby(TRAIN_LEVEL)\n",
    "    .apply(\n",
    "        train_predict_sktime,\n",
    "        FEATRURE_COLUMNS,\n",
    "        TARGET_COLUMNS,\n",
    "        max_lag=365,\n",
    "        pipeline=pipeline,\n",
    "        pipeline_lags_transformer=lag_window_feats_3,\n",
    "        imputation=\"mean\",\n",
    "    )\n",
    "    .drop(columns=[\"warehouse\"])\n",
    "    .reset_index()\n",
    "    .query(\"missing==0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Batch training without lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(f\"split=='train' and not {TARGET_COLUMNS}.isnull()\").groupby(\n",
    "    TRAIN_LEVEL\n",
    ").apply(\n",
    "    train_model_core,\n",
    "    TRAIN_LEVEL,\n",
    "    FEATRURE_COLUMNS,\n",
    "    TARGET_COLUMNS,\n",
    "    pipeline,\n",
    "    model_dir=MODEL_PATH,\n",
    "    include_groups=True,\n",
    ").reset_index(\n",
    "    drop=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_4 = (\n",
    "    df.query(f\"split=='test'\")\n",
    "    .reset_index()\n",
    "    .groupby(TRAIN_LEVEL)\n",
    "    .apply(\n",
    "        score_model_core,\n",
    "        TRAIN_LEVEL,\n",
    "        FEATRURE_COLUMNS,\n",
    "        TARGET_COLUMNS,\n",
    "        model_dir=MODEL_PATH,\n",
    "        include_groups=True,\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    "    .query(\"missing==0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Evaluation of models against holdout period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = (\n",
    "    df_forecast[[\"id\", \"warehouse\", \"date\", \"orders\", \"orders_copy\"]]\n",
    "    .merge(\n",
    "        df_forecast_2[[\"id\", \"warehouse\", \"date\", \"orders\"]].rename(\n",
    "            columns={\"orders\": \"orders_2\"}\n",
    "        ),\n",
    "        on=[\"id\", \"warehouse\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .merge(\n",
    "        df_forecast_3[[\"id\", \"warehouse\", \"date\", \"orders\"]].rename(\n",
    "            columns={\"orders\": \"orders_3\"}\n",
    "        ),\n",
    "        on=[\"id\", \"warehouse\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .merge(\n",
    "        df_forecast_4[[\"id\", \"warehouse\", \"date\", \"forecast\"]],\n",
    "        on=[\"id\", \"warehouse\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "df_final[\"ensemble\"] = df_final[[\"orders\", \"orders_2\", \"orders_3\", \"forecast\"]].mean(\n",
    "    axis=1\n",
    ")\n",
    "df_final[\"ensemble_2\"] = df_final[[\"orders\", \"orders_3\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wmape(df_forecast[\"orders_copy\"], df_forecast[\"orders\"]))\n",
    "print(wmape(df_forecast_2[\"orders_copy\"], df_forecast_2[\"orders\"]))\n",
    "print(wmape(df_forecast_3[\"orders_copy\"], df_forecast_3[\"orders\"]))\n",
    "print(wmape(df_forecast_4[\"orders_copy\"], df_forecast_4[\"forecast\"]))\n",
    "print(wmape(df_final[\"orders_copy\"], df_final[\"ensemble\"]))\n",
    "print(wmape(df_final[\"orders_copy\"], df_final[\"ensemble_2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mape(df_forecast[\"orders_copy\"], df_forecast[\"orders\"]))\n",
    "print(mape(df_forecast_2[\"orders_copy\"], df_forecast_2[\"orders\"]))\n",
    "print(mape(df_forecast_3[\"orders_copy\"], df_forecast_3[\"orders\"]))\n",
    "print(mape(df_forecast_4[\"orders_copy\"], df_forecast_4[\"forecast\"]))\n",
    "print(mape(df_final[\"orders_copy\"], df_final[\"ensemble\"]))\n",
    "print(mape(df_final[\"orders_copy\"], df_final[\"ensemble_2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warehouse= \"Prague_3\"\n",
    "for warehouse in df.warehouse.unique():\n",
    "    fig, ax = plt.subplots(figsize=[25, 7])\n",
    "    min_date = df.query(\n",
    "        f\"split=='train' and warehouse=='{warehouse}'\"\n",
    "    ).index.max() - datetime.timedelta(days=365)\n",
    "    # filter dates after min_date\n",
    "    # df.query(f\"split=='train' and warehouse=='{warehouse}'\").loc[min_date:].plot(y=\"orders\", marker=\".\", figsize=[25, 8], legend=None, ax=ax)\n",
    "    df.query(f\"warehouse=='{warehouse}'\").loc[min_date:].plot(\n",
    "        y=\"orders_copy\", marker=\".\", figsize=[25, 8], legend=None, ax=ax\n",
    "    )\n",
    "    df_forecast.query(f\"split=='test' and warehouse=='{warehouse}'\").set_index(\n",
    "        \"date\"\n",
    "    ).plot(y=\"orders\", marker=\".\", figsize=[25, 8], legend=None, ax=ax)\n",
    "    df_forecast_2.query(f\"split=='test' and warehouse=='{warehouse}'\").set_index(\n",
    "        \"date\"\n",
    "    ).plot(y=\"orders\", marker=\".\", figsize=[25, 8], legend=None, ax=ax)\n",
    "    df_forecast_3.query(f\"split=='test' and warehouse=='{warehouse}'\").set_index(\n",
    "        \"date\"\n",
    "    ).plot(y=\"orders\", marker=\".\", figsize=[25, 8], legend=None, ax=ax)\n",
    "    df_forecast_4.query(f\"split=='test' and warehouse=='{warehouse}'\").set_index(\n",
    "        \"date\"\n",
    "    ).plot(y=\"forecast\", marker=\".\", figsize=[25, 8], legend=None, ax=ax)\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Demand\")\n",
    "    ax.set_title(\"Forecast for {}\".format(warehouse))\n",
    "    ax.legend([\"Actuals\", \"forecast_1\", \"forecast_2\", \"forecast_3\", \"forecast_4\"])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Forecasting for test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast = (\n",
    "    df.groupby(TRAIN_LEVEL)\n",
    "    .apply(\n",
    "        train_predict_sktime,\n",
    "        FEATRURE_COLUMNS,\n",
    "        TARGET_COLUMNS,\n",
    "        max_lag=365,\n",
    "        pipeline=pipeline,\n",
    "        pipeline_lags_transformer=lag_window_feats,\n",
    "        imputation=\"mean\",\n",
    "    )\n",
    "    .drop(columns=[\"warehouse\"])\n",
    "    .reset_index()\n",
    "    .query(\"missing==0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_2 = (\n",
    "    df.groupby(TRAIN_LEVEL)\n",
    "    .apply(\n",
    "        train_predict_sktime,\n",
    "        FEATRURE_COLUMNS,\n",
    "        TARGET_COLUMNS,\n",
    "        max_lag=7,\n",
    "        pipeline=pipeline,\n",
    "        pipeline_lags_transformer=lag_window_feats_2,\n",
    "        imputation=\"mean\",\n",
    "    )\n",
    "    .drop(columns=[\"warehouse\"])\n",
    "    .reset_index()\n",
    "    .query(\"missing==0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast_3 = (\n",
    "    df.groupby(TRAIN_LEVEL)\n",
    "    .apply(\n",
    "        train_predict_sktime,\n",
    "        FEATRURE_COLUMNS,\n",
    "        TARGET_COLUMNS,\n",
    "        max_lag=365,\n",
    "        pipeline=pipeline,\n",
    "        pipeline_lags_transformer=lag_window_feats_3,\n",
    "        imputation=\"mean\",\n",
    "    )\n",
    "    .drop(columns=[\"warehouse\"])\n",
    "    .reset_index()\n",
    "    .query(\"missing==0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query(f\"split=='train' and not {TARGET_COLUMNS}.isnull()\").groupby(\n",
    "    TRAIN_LEVEL\n",
    ").apply(\n",
    "    train_model_core,\n",
    "    TRAIN_LEVEL,\n",
    "    FEATRURE_COLUMNS,\n",
    "    TARGET_COLUMNS,\n",
    "    pipeline,\n",
    "    model_dir=MODEL_PATH,\n",
    "    include_groups=True,\n",
    ").reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "df_forecast_4 = (\n",
    "    df.query(f\"split=='test'\")\n",
    "    .reset_index()\n",
    "    .groupby(TRAIN_LEVEL)\n",
    "    .apply(\n",
    "        score_model_core,\n",
    "        TRAIN_LEVEL,\n",
    "        FEATRURE_COLUMNS,\n",
    "        TARGET_COLUMNS,\n",
    "        model_dir=MODEL_PATH,\n",
    "        include_groups=True,\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    "    .query(\"missing==0\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = (\n",
    "    df_forecast[[\"id\", \"warehouse\", \"date\", \"orders\"]]\n",
    "    .merge(\n",
    "        df_forecast_2[[\"id\", \"warehouse\", \"date\", \"orders\"]].rename(\n",
    "            columns={\"orders\": \"orders_2\"}\n",
    "        ),\n",
    "        on=[\"id\", \"warehouse\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .merge(\n",
    "        df_forecast_3[[\"id\", \"warehouse\", \"date\", \"orders\"]].rename(\n",
    "            columns={\"orders\": \"orders_3\"}\n",
    "        ),\n",
    "        on=[\"id\", \"warehouse\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .merge(\n",
    "        df_forecast_4[[\"id\", \"warehouse\", \"date\", \"forecast\"]],\n",
    "        on=[\"id\", \"warehouse\", \"date\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    ")\n",
    "df_final[\"ensemble\"] = df_final[[\"orders\", \"orders_2\", \"orders_3\", \"forecast\"]].mean(\n",
    "    axis=1\n",
    ")\n",
    "df_final[\"ensemble_2\"] = df_final[[\"orders\", \"orders_3\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"../data/submission/\"\n",
    "df_final.to_csv(f\"{PATH}all_sumissions.csv\", index=False)\n",
    "df_final[[\"id\", \"orders\"]].to_csv(f\"{PATH}sumission_1.csv\", index=False)\n",
    "df_final[[\"id\", \"orders_2\"]].rename(columns={\"orders_2\": \"orders\"}).to_csv(\n",
    "    f\"{PATH}sumission_2.csv\", index=False\n",
    ")\n",
    "df_final[[\"id\", \"orders_3\"]].rename(columns={\"orders_3\": \"orders\"}).to_csv(\n",
    "    f\"{PATH}sumission_3.csv\", index=False\n",
    ")\n",
    "df_final[[\"id\", \"forecast\"]].rename(columns={\"forecast\": \"orders\"}).to_csv(\n",
    "    f\"{PATH}sumission_4.csv\", index=False\n",
    ")\n",
    "df_final[[\"id\", \"ensemble\"]].rename(columns={\"ensemble\": \"orders\"}).to_csv(\n",
    "    f\"{PATH}sumission_5.csv\", index=False\n",
    ")\n",
    "df_final[[\"id\", \"ensemble_2\"]].rename(columns={\"ensemble_2\": \"orders\"}).to_csv(\n",
    "    f\"{PATH}sumission_6.csv\", index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
